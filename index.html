<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resource/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>LightningDrag: Lightning Fast and Accurate Drag-based Image Editing Emerging from Videos</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="Path to my teaser.jpg"/>
  	<meta property="og:title" content="LightningDrag: Lightning Fast and Accurate Drag-based Image Editing Emerging from Videos" />
  	<meta property="og:description" content="Paper description." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="LightningDrag: Lightning Fast and Accurate Drag-based Image Editing Emerging from Videos" />
    <meta property="twitter:description"   content="" />
    <meta property="twitter:image"         content="Path to my teaser.jpg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title" display="block">
        <b>
        LightningDrag: Lightning Fast and Accurate
        <br>
        Drag-based Image Editing Emerging from Videos
        </b>
    </div>
    
    <br><br><br>
    <div class="author">
        <img width="128" src="./resource/yujun/edit.png"/>
        <img width="128" src="./resource/yujun.gif"/>
        <br>
        <a href="https://yujun-shi.github.io/">Yujun Shi</a><sup>1</sup><sup>*</sup>
    </div>
    <div class="author">
        <img width="128" src="./resource/junhao/edit.png"/>
        <img width="128" src="./resource/junhao.gif"/>
        <br>
        <a href="https://scholar.google.com.sg/citations?user=8gm-CYYAAAAJ&hl=en">Jun Hao Liew</a><sup>2</sup><sup>*^</sup>
    </div>
    <div class="author">
        <img width="128" src="./resource/hanshu/edit.png"/>
        <img width="128" src="./resource/hanshu.gif"/>
        <br>
        <a href="https://hanshuyan.github.io/">Hanshu Yan</a><sup>2</sup>
    </div>
    <br>
    <br>
    <div class="author">
        <img width="128" src="./resource/vincent/edit.png"/>
        <img width="128" src="./resource/vincent.gif"/>
        <br>
        <a href="https://vyftan.github.io/">Vincent Y. F. Tan</a><sup>1</sup>
    </div>
    <div class="author">
        <img width="128" src="./resource/jiashi/edit.png"/>
        <img width="128" src="./resource/jiashi.gif"/>
        <br>
        <a href="https://sites.google.com/site/jshfeng/home">Jiashi Feng</a><sup>2</sup>
    </div>

    <br><br><br>

    <div class="affiliation">
        <sup>1&nbsp;</sup>National University of Singapore&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>2&nbsp;</sup>ByteDance
    </div>

    <br><br>
    <div class="affiliation">
        <sup>*&nbsp;</sup>Equal Contributions&nbsp;&nbsp;&nbsp;&nbsp;<sup>^&nbsp;</sup>Project Lead
    </div>


    <br><br>

    <!--
    <div class="links"><a href="https://arxiv.org/abs/2306.14435">[Paper]</a></div>
    <div class="links"><a href="https://github.com/Yujun-Shi/DragDiffusion">[Code]</a></div>
    <div class="links"><a href="https://www.bilibili.com/video/BV1wX4y1p7Ns/?vd_source=fa65719b2d4e7994ab64a631336a3a99#reply360690777">[Video]</a></div>
    -->

    <br><br>
    <div class="links"><a href="https://arxiv.org/abs/2405.13722">[Paper]</a></div>
    <div class="links"><a href="https://github.com/magic-research/LightningDrag">[Code]</a></div>
    <br><br>

    <h1>Abstract</h1>
    <p style="width: 80%;text-align: justify">
        Accuracy and speed are critical in image editing tasks.
        Pan et al. introduced a drag-based image editing framework that achieves
        pixel-level control using Generative Adversarial Networks (GANs).
        A flurry of subsequent studies enhanced this framework's generality by
        leveraging large-scale diffusion models. However, these methods often
        suffer from inordinately long processing times (exceeding 1 minute per edit) and
        low success rates. Addressing these issues head on, we present LightningDrag,
        a rapid approach enabling high quality drag-based image editing in ~1 second.
        Unlike most previous methods, we redefine drag-based editing as a conditional
        generation task, eliminating the need for time-consuming latent optimization
        or gradient-based guidance during inference.
        In addition, the design of our pipeline allows us to train our model on
        large-scale paired video frames, which contain rich motion information
        such as object translations, changing poses and orientations, zooming in and out, etc.
        By learning from videos, our approach can significantly outperform previous methods
        in terms of accuracy and consistency. Despite being trained solely on videos,
        our model generalizes well to perform local shape deformations not presented
        in the training data (e.g., lengthening of hair, twisting rainbows, etc.).
        Extensive qualitative and quantitative evaluations on benchmark datasets corroborate
        the superiority of our approach. The code and model will be released.
    </p>

    <br>
    <hr>

    <br>
    <img style="width: 80%;" src="./resource/teaser.png"/>
    <br>
    <p style="width: 75%;text-align: justify">
        The user provides handle points (<span style="color: red;">red</span>),
        target points (<span style="color: blue;">blue</span>),
        and a mask specifying the editable region
        (<span style="color: gray;">brighter area</span>).
        Lower Mean Distance indicates more effective "draggging",
        while higher Image Fidelity implies better appearance preserving.
        Our approach significantly surpass the previous methods in terms of
        both speed and quality. Image credit (source images): Pexels.
    </p>
    <br>
    <hr>

    <br>
    <div class="video-with-text">
        <h1>Live Demo</h1>
        <br>
        <video style="width: 75%" controls>
            <source src="./resource/demo_cover.mp4" type="video/mp4">
        </video>
    </div>
    <br>
    <hr>

    <br>
    <h1>More Qualitative Results</h1>

    <h2>Comparisons with Prior Methods</h2>
    <img style="width: 80%;" src="./resource/qual_comparison.png"
         alt="Results figure"/>
    <br>

    <h2>Single-Round Dragging</h2>
    <img style="width: 80%;" src="./resource/single_round.png"
         alt="Results figure"/>
    <br>

    <h2>Multi-Round Dragging</h2>
    <img style="width: 80%;" src="./resource/multi_round.png"
         alt="Results figure"/>

    <br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a> and <a href="https://github.com/elliottwu/webpage-template">Elliott Wu</a>.
    </p>

    <br><br>
</div>

</body>

</html>
